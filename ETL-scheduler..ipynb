{
  "metadata": {
    "name": "ETL-scheduler",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.pyspark\nfrom datetime import datetime, timedelta\n\ntoday \u003d datetime.today().date()\nyesterday \u003d today - timedelta(days\u003d1)\n\n# menampilkan tanggal kemarin dalam format yyyy-mm-dd\nprint(yesterday.strftime(\u0027%Y-%m-%d\u0027))\n\nprint(\"Today\u0027s date:\", today)\nprint(\"Yesterday\u0027s date:\", yesterday)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.pyspark\n\nsqlquery \u003d f\"\"\"SELECT \n  GJH.JE_HEADER_ID,  \n  GJH.BATCH_NAME, \n  GJH.NAME JOURNAL_NAME, \n  GJH.JE_SOURCE, \n  GJH.JE_CATEGORY, \n  GJH.DESCRIPTION HDR_DESCRIPTION, \n  GJH.BATCH_POSTED_DATE, \n  GJH.CREATION_DATE, \n  GJH.CURRENCY_CODE, \n  GJH.RUNNING_TOTAL_DR, \n  GJH.RUNNING_TOTAL_CR, \n  GJH.RUNNING_TOTAL_ACCOUNTED_DR, \n  GJH.RUNNING_TOTAL_ACCOUNTED_CR, \n  GJH.LEDGER_ID, \n  GJH.PERIOD_NAME, \n  GJH.STATUS HDR_STATUS, \n  GJL.JE_LINE_NUM, \n  GJL.SEGMENT2 KODE_PP, \n  GJL.SEGMENT3 ACCOUNT, \n  GJL.SEGMENT4 SUB_ACCOUNT, \n  CASE WHEN GJL.SEGMENT4 \u003d \u002700\u0027 THEN (\n    SELECT \n      FFVL.DESCRIPTION \n    FROM \n      FND_FLEX_VALUES_VL FFVL \n    WHERE \n      FFVL.FLEX_VALUE_SET_ID \u003d 1014176 \n      AND FFVL.FLEX_VALUE \u003d GJL.SEGMENT3\n  ) ELSE (\n    SELECT \n      FFVL.DESCRIPTION \n    FROM \n      FND_FLEX_VALUES_VL FFVL \n    WHERE \n      FFVL.FLEX_VALUE_SET_ID \u003d 1014176 \n      AND FFVL.FLEX_VALUE \u003d GJL.SEGMENT3\n  )|| \u0027-\u0027 || (\n    SELECT \n      FFVL.DESCRIPTION \n    FROM \n      FND_FLEX_VALUES_VL FFVL \n    WHERE \n      FFVL.FLEX_VALUE_SET_ID \u003d 1014177 \n      AND FFVL.PARENT_FLEX_VALUE_LOW \u003d GJL.SEGMENT3 \n      AND FFVL.FLEX_VALUE \u003d GJL.SEGMENT4\n  ) END ACCOUNT_DESC, \n  (\n    GJL.SEGMENT1 || \u0027.\u0027 || GJL.SEGMENT2 || \u0027.\u0027 || GJL.SEGMENT3 || \u0027.\u0027 || GJL.SEGMENT4 || \u0027.\u0027 || GJL.SEGMENT5 || \u0027.\u0027 || GJL.SEGMENT6\n  ) GL_ACCOUNT, \n  GJL.DESCRIPTION LINE_DESCRIPTION, \n  GJL.ACCOUNTED_DR, \n  GJL.ACCOUNTED_CR, \n  GJL.STATUS, \n  FU.USER_NAME ENTERED_BY \nFROM \n  GL_JE_HEADERS_V GJH, \n  GL_JE_LINES_V GJL, \n  GL_JE_BATCHES GJB, \n  FND_USER FU, \n  GL_LEDGERS GL \nWHERE \n  GJH.LEDGER_ID \u003d GL.LEDGER_ID \n  AND GJH.JE_HEADER_ID \u003d GJL.JE_HEADER_ID \n  AND GJH.JE_BATCH_ID \u003d GJB.JE_BATCH_ID (+) \n  AND GJH.CREATED_BY \u003d FU.USER_ID \n  AND GJH.STATUS \u003d \u0027P\u0027 \n  AND trunc(GJH.BATCH_POSTED_DATE) between to_date(\u0027{yesterday}\u0027, \u0027YYYY-MM-DD\u0027) and to_date(\u0027{yesterday}\u0027, \u0027YYYY-MM-DD\u0027)\"\"\""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.pyspark\n\njdbcDF \u003d spark.read.format(\u0027jdbc\u0027).option(\u0027url\u0027, \u0027jdbc:oracle:thin:@10.206.130.140:1541/PRDGG\u0027)\\\n    .option(\u0027user\u0027, \u0027GGGGAPPSRO\u0027)\\\n    .option(\u0027dbtable\u0027,f\"({sqlquery})\") \\\n    .option(\u0027password\u0027, \u0027k3d1r1\u0027)\\\n    .option(\u0027driver\u0027, \u0027oracle.jdbc.driver.OracleDriver\u0027)\\\n    .load()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.pyspark\nfrom pyspark.sql.types import StringType, DateType, FloatType, DecimalType, IntegerType\n\nfor column in jdbcDF.columns:\n    jdbcDF \u003d jdbcDF.withColumnRenamed(column, column.lower())\n\njdbcDF \u003d jdbcDF \\\n    .withColumn(\"je_header_id\" , jdbcDF[\"je_header_id\"].cast(StringType()))\\\n    .withColumn(\"running_total_dr\" , jdbcDF[\"running_total_dr\"].cast(DecimalType(38, 18)))\\\n    .withColumn(\"running_total_cr\" , jdbcDF[\"running_total_cr\"].cast(DecimalType(38, 18)))\\\n    .withColumn(\"running_total_accounted_dr\" , jdbcDF[\"running_total_accounted_dr\"].cast(DecimalType(38, 18)))\\\n    .withColumn(\"running_total_accounted_cr\" , jdbcDF[\"running_total_accounted_cr\"].cast(DecimalType(38, 18)))\\\n    .withColumn(\"ledger_id\" , jdbcDF[\"ledger_id\"].cast(StringType()))\\\n    .withColumn(\"je_line_num\" , jdbcDF[\"je_line_num\"].cast(StringType()))\\\n    .withColumn(\"accounted_dr\" , jdbcDF[\"accounted_dr\"].cast(DecimalType(38, 18)))\\\n    .withColumn(\"accounted_cr\" , jdbcDF[\"accounted_cr\"].cast(DecimalType(38, 18)))\n    \njdbcDF.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.pyspark\n\njdbcDF.write.format(\"parquet\") \\\n    .option(\"transactional\", \"false\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"dev_gl_storage.dev_gl\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.sql\n\nDROP TABLE if exists datamart_gl_budget.GL_AGGS ;\n\nCREATE TABLE datamart_gl_budget.GL_AGGS AS\nSELECT \u002720\u0027||SUBSTR(PERIOD_NAME,-2) YEAR\n ,CASE SUBSTR(PERIOD_NAME,1,3)\n WHEN \u0027JAN\u0027 THEN \u002701\u0027\n WHEN \u0027FEB\u0027 THEN \u002702\u0027\n WHEN \u0027MAR\u0027 THEN \u002703\u0027\n WHEN \u0027APR\u0027 THEN \u002704\u0027\n WHEN \u0027MAY\u0027 THEN \u002705\u0027\n WHEN \u0027JUN\u0027 THEN \u002706\u0027\n WHEN \u0027JUL\u0027 THEN \u002707\u0027\n WHEN \u0027AUG\u0027 THEN \u002708\u0027\n WHEN \u0027SEP\u0027 THEN \u002709\u0027\n WHEN \u0027OCT\u0027 THEN \u002710\u0027\n WHEN \u0027NOV\u0027 THEN \u002711\u0027\n WHEN \u0027DEC\u0027 THEN \u002712\u0027\n END MONTH\n ,KODE_PP\n ,ACCOUNT\n ,COUNT(1) ROW_COUNT\n ,SUM(coalesce(ACCOUNTED_DR,0)) AS TOTAL_ACCOUNTED_DR\n ,SUM(coalesce(ACCOUNTED_CR,0)) AS TOTAL_ACCOUNTED_CR\n FROM dev_gl_storage.dev_gl\nGROUP BY \u002720\u0027||SUBSTR(PERIOD_NAME,-2)\n ,CASE SUBSTR(PERIOD_NAME,1,3)\n WHEN \u0027JAN\u0027 THEN \u002701\u0027\n WHEN \u0027FEB\u0027 THEN \u002702\u0027\n WHEN \u0027MAR\u0027 THEN \u002703\u0027\n WHEN \u0027APR\u0027 THEN \u002704\u0027\n WHEN \u0027MAY\u0027 THEN \u002705\u0027\n WHEN \u0027JUN\u0027 THEN \u002706\u0027\n WHEN \u0027JUL\u0027 THEN \u002707\u0027\n WHEN \u0027AUG\u0027 THEN \u002708\u0027\n WHEN \u0027SEP\u0027 THEN \u002709\u0027\n WHEN \u0027OCT\u0027 THEN \u002710\u0027\n WHEN \u0027NOV\u0027 THEN \u002711\u0027\n WHEN \u0027DEC\u0027 THEN \u002712\u0027\n END\n ,KODE_PP\n ,ACCOUNT"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.sql\n\nDROP TABLE if exists datamart_gl_budget.GL_FACT;\n\nCREATE TABLE datamart_gl_budget.GL_FACT AS\nSELECT gl.year\n ,gl.month\n ,gl.kode_pp\n ,gl.account\n ,coalesce(gl.total_accounted_dr, 0) total_accounted_dr\n ,coalesce(gl.total_accounted_cr, 0) total_accounted_cr\n ,coalesce(bg.budget, 0) total_budget\n ,coalesce(coalesce(gl.total_accounted_dr-gl.total_accounted_cr, 0)) total_expense\n ,coalesce(coalesce(bg.budget, 0)-(coalesce(gl.total_accounted_dr, 0)-coalesce(gl.total_accounted_cr, 0)),0) available\n FROM datamart_gl_budget.gl_aggs gl\nFULL JOIN budget.budget_transform bg\n ON (gl.month\u003dbg.month AND gl.kode_pp\u003dbg.department_id AND\ngl.account\u003dbg.account_id AND gl.year\u003dbg.year);"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.sql\n\ndrop table if exists datamart_gl_budget.olap_cube;\nCREATE TABLE datamart_gl_budget.OLAP_CUBE AS SELECT to_date\n(from_unixtime(unix_timestamp(glf.year||\u0027-\u0027||glf.month||\u0027-\u0027||\u002701\u0027,\u0027yyyy-MM-dd\u0027))) as period_name\n ,glf.kode_pp\n ,glf.year\n ,glf.month\n ,dept.name AS department_name\n ,glf.account\n ,act.name AS account_name\n ,glf.total_accounted_dr\n ,glf.total_accounted_cr\n ,glf.total_budget\n ,glf.total_expense\n ,glf.available\n FROM datamart_gl_budget.gl_fact glf\nLEFT JOIN dimension.department dept\n ON (glf.kode_pp\u003ddept.id)\nLEFT JOIN dimension.account act\n ON (glf.account\u003dact.id);"
    }
  ]
}